{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d91cd1e8-9db6-4cb8-82c7-96bb487fefda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: AUC\n",
      "Epoch: 00, Loss: 0.6941, Valid: 58.81%, Test: 54.64%\n",
      "AUC\n",
      "Epoch: 01, Loss: 0.6928, Valid: 63.00%, Test: 59.05%\n",
      "AUC\n",
      "Epoch: 02, Loss: 0.6904, Valid: 65.45%, Test: 63.14%\n",
      "AUC\n",
      "Epoch: 03, Loss: 0.6858, Valid: 66.56%, Test: 65.85%\n",
      "AUC\n",
      "Epoch: 04, Loss: 0.6712, Valid: 67.09%, Test: 67.34%\n",
      "AUC\n",
      "Epoch: 05, Loss: 0.6530, Valid: 67.64%, Test: 68.08%\n",
      "AUC\n",
      "Epoch: 06, Loss: 0.6415, Valid: 67.99%, Test: 68.30%\n",
      "AUC\n",
      "Epoch: 07, Loss: 0.6332, Valid: 68.30%, Test: 68.61%\n",
      "AUC\n",
      "Epoch: 08, Loss: 0.6220, Valid: 68.57%, Test: 68.92%\n",
      "AUC\n",
      "Epoch: 09, Loss: 0.6128, Valid: 69.10%, Test: 69.27%\n",
      "\n",
      "Error: /home/antoni_valls/beegfs/miniconda3/envs/lp_env/lib/python3.12/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "/home/antoni_valls/beegfs/miniconda3/envs/lp_env/lib/python3.12/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "/home/antoni_valls/beegfs/miniconda3/envs/lp_env/lib/python3.12/site-packages/torch_geometric/io/fs.py:215: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location)\n",
      "/mnt/beegfs/iiia/antoni_valls/Link-Prediction-Bias/utils/seal_datasets.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python cluster_experiments.py --model_name 'seal' --tgm_type 'Planetoid' --name 'cora' --epochs 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59725f26-7acd-4f7e-b89f-f82d4c7eb298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoni_valls/beegfs/miniconda3/envs/lp_env/lib/python3.12/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "/home/antoni_valls/beegfs/miniconda3/envs/lp_env/lib/python3.12/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "/home/antoni_valls/beegfs/miniconda3/envs/lp_env/lib/python3.12/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "/home/antoni_valls/beegfs/miniconda3/envs/lp_env/lib/python3.12/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "/home/antoni_valls/beegfs/miniconda3/envs/lp_env/lib/python3.12/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "/home/antoni_valls/beegfs/miniconda3/envs/lp_env/lib/python3.12/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC\n",
      "Epoch: 00, Loss: 0.6941, Valid: 58.81%, Test: 54.64%\n",
      "AUC\n",
      "Epoch: 01, Loss: 0.6928, Valid: 63.00%, Test: 59.05%\n",
      "AUC\n",
      "Epoch: 02, Loss: 0.6904, Valid: 65.45%, Test: 63.14%\n",
      "AUC\n",
      "Epoch: 03, Loss: 0.6858, Valid: 66.56%, Test: 65.85%\n",
      "AUC\n",
      "Epoch: 04, Loss: 0.6712, Valid: 67.09%, Test: 67.34%\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import argparse\n",
    "import scipy.sparse as ssp\n",
    "from tqdm import tqdm\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from models.train_gnn import train_and_predict\n",
    "from torch_geometric import seed_everything\n",
    "from utils.seal_utils import *\n",
    "from utils.data_utils import data_loader\n",
    "from utils.seal_datasets import SEALDataset\n",
    "from models.gcn import GCN, GCN_woBatch\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from scipy.sparse import SparseEfficiencyWarning\n",
    "warnings.simplefilter('ignore', SparseEfficiencyWarning)\n",
    "\n",
    "def train_wBatch():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x = data.x\n",
    "        edge_weight = None\n",
    "        node_id = data.node_id if emb else None\n",
    "        logits = model(data.z, data.edge_index, data.batch, x, edge_weight, node_id)\n",
    "        loss = BCEWithLogitsLoss()(logits.view(-1), data.y.to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_dataset)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    x = train_dataset.x \n",
    "    edge_weight =  None\n",
    "    node_id = train_dataset.node_id if emb else None\n",
    "    logits = model(train_dataset.z, train_dataset.data.edge_label_index, x, edge_weight, node_id)\n",
    "    loss = BCEWithLogitsLoss()(logits.view(-1), train_dataset.edge_label.to(torch.float))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_wBatch():\n",
    "    model.eval()\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    for data in val_loader:\n",
    "        data = data.to(device)\n",
    "        x = data.x \n",
    "        edge_weight = None\n",
    "        node_id = data.node_id if emb else None\n",
    "        logits = model(data.z, data.edge_index, data.batch, x, edge_weight, node_id)\n",
    "        y_pred.append(logits.view(-1).cpu())\n",
    "        y_true.append(data.y.view(-1).cpu().to(torch.float))\n",
    "    val_pred, val_true = torch.cat(y_pred), torch.cat(y_true)\n",
    "    pos_val_pred = val_pred[val_true==1]\n",
    "    neg_val_pred = val_pred[val_true==0]\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        x = data.x \n",
    "        edge_weight =  None\n",
    "        node_id = data.node_id if emb else None\n",
    "        logits = model(data.z, data.edge_index, data.batch, x, edge_weight, node_id)\n",
    "        y_pred.append(logits.view(-1).cpu())\n",
    "        y_true.append(data.y.view(-1).cpu().to(torch.float))\n",
    "    test_pred, test_true = torch.cat(y_pred), torch.cat(y_true)\n",
    "    pos_test_pred = test_pred[test_true==1]\n",
    "    neg_test_pred = test_pred[test_true==0]\n",
    "    \n",
    "    results = evaluate_auc(val_pred, val_true, test_pred, test_true)\n",
    "\n",
    "    return results\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    x = val_dataset.x \n",
    "    edge_weight =  None\n",
    "    node_id = val_dataset.node_id if emb else None\n",
    "    logits = model(val_dataset.z, val_dataset.data.edge_label_index, x, edge_weight, node_id)\n",
    "    val_pred = logits.view(-1).cpu()\n",
    "    val_true = val_dataset.y.view(-1).cpu().to(torch.float)\n",
    "    pos_val_pred = val_pred[val_true==1]\n",
    "    neg_val_pred = val_pred[val_true==0]\n",
    "\n",
    "    x = test_dataset.x \n",
    "    edge_weight = None\n",
    "    node_id = test_dataset.node_id if emb else None\n",
    "    logits = model(test_dataset.z, test_dataset.data.edge_label_index, x, edge_weight, node_id)\n",
    "    test_pred = logits.view(-1).cpu()\n",
    "    test_true = test_dataset.y.view(-1).cpu().to(torch.float)\n",
    "    pos_test_pred = test_pred[test_true==1]\n",
    "    neg_test_pred = test_pred[test_true==0]\n",
    "    \n",
    "    results = evaluate_auc(val_pred, val_true, test_pred, test_true)\n",
    "\n",
    "    return results\n",
    "\n",
    "# check device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# set seed\n",
    "seed_everything(60)\n",
    "\n",
    "# Load dataset\n",
    "dataset = data_loader(tgm_type=\"Planetoid\", name=\"cora\", transform=None)\n",
    "split_edge = do_edge_split(dataset, True)\n",
    "data = dataset[0]\n",
    "data.edge_index = split_edge['train']['edge'].t()\n",
    "directed = False\n",
    "\n",
    "# convert the data in seal_datasets, i.e. with the subgraphs done\n",
    "path = f\"data/Planetoid/cora/SEAL\"\n",
    "dataset_class = 'SEALDataset'\n",
    "train_dataset = eval(dataset_class)(\n",
    "    path, \n",
    "    data, \n",
    "    split_edge, \n",
    "    num_hops=1, \n",
    "    percent=100, \n",
    "    split='train') \n",
    "if False:  # visualize some graphs\n",
    "    import networkx as nx\n",
    "    from torch_geometric.utils import to_networkx\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "    for g in loader:\n",
    "        f = plt.figure(figsize=(20, 20))\n",
    "        limits = plt.axis('off')\n",
    "        g = g.to(device)\n",
    "        node_size = 100\n",
    "        with_labels = True\n",
    "        G = to_networkx(g, node_attrs=['z'])\n",
    "        labels = {i: G.nodes[i]['z'] for i in range(len(G))}\n",
    "        nx.draw(G, node_size=node_size, arrows=True, with_labels=with_labels,\n",
    "                labels=labels)\n",
    "        f.savefig('tmp_vis.png')\n",
    "        pdb.set_trace()\n",
    "\n",
    "val_dataset = eval(dataset_class)(\n",
    "    path, \n",
    "    data, \n",
    "    split_edge, \n",
    "    num_hops=1, \n",
    "    percent=100, \n",
    "    split='valid')\n",
    "\n",
    "test_dataset = eval(dataset_class)(\n",
    "    path, \n",
    "    data, \n",
    "    split_edge, \n",
    "    num_hops=1, \n",
    "    percent=100, \n",
    "    split='test')\n",
    "    \n",
    "train_loader = DataLoader(train_dataset, batch_size=32, \n",
    "                           shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# define the training process\n",
    "emb = None\n",
    "max_z = 1000  # set a large max_z so that every z has embeddings to look up\n",
    "model = GCN(32, 2, max_z, train_dataset, \n",
    "                True).to(device)\n",
    "parameters = list(model.parameters())\n",
    "optimizer = torch.optim.Adam(params=parameters, lr=0.0001)\n",
    "\n",
    "for epoch in range(1):\n",
    "    loss = train_wBatch()\n",
    "    results = test_wBatch()\n",
    "    for key, result in results.items():\n",
    "        valid_res, test_res = result\n",
    "        to_print = (f'Epoch: {epoch:02d}, ' +\n",
    "                    f'Loss: {loss:.4f}, Valid: {100 * valid_res:.2f}%, ' +\n",
    "                    f'Test: {100 * test_res:.2f}%')\n",
    "        print(key)\n",
    "        print(to_print)\n",
    "\n",
    "# get the graph results\n",
    "train_data = next(iter(DataLoader(train_dataset, batch_size=len(train_dataset)))).to(device)\n",
    "val_data = next(iter(DataLoader(val_dataset, batch_size=len(val_dataset)))).to(device)\n",
    "test_data = next(iter(DataLoader(test_dataset, batch_size=len(test_dataset)))).to(device)\n",
    "\n",
    "G = to_networkx(train_data, to_undirected=True)\n",
    "\n",
    "val_logits = model(val_data.z, val_data.edge_index, val_data.batch, val_data.x, None, val_data.node_id)\n",
    "val_predictions = val_logits.view(-1).sigmoid().detach().cpu().numpy()\n",
    "val_label = val_data.y.view(-1).cpu().numpy().astype(\"float32\")\n",
    "pos_val_edges, neg_val_edges = get_pos_neg_edges('valid', split_edge, val_data.edge_index, val_data.num_nodes, percent=100)\n",
    "val_edges = torch.cat((pos_val_edges, neg_val_edges), dim=1).numpy()\n",
    "\n",
    "test_logits = model(test_data.z, test_data.edge_index, test_data.batch, test_data.x, None, test_data.node_id)\n",
    "test_predictions = test_logits.view(-1).sigmoid().detach().cpu().numpy()\n",
    "test_label = test_data.y.view(-1).cpu().numpy().astype(\"float32\")\n",
    "pos_test_edges, neg_test_edges = get_pos_neg_edges('test', split_edge, test_data.edge_index, test_data.num_nodes, percent=100)\n",
    "test_edges = torch.cat((pos_test_edges, neg_test_edges), dim=1).numpy()\n",
    "\n",
    "\n",
    "results = {\"train_graph\": G,\n",
    "        \"test_predictions\": test_predictions,\n",
    "        \"test_labels\": test_label,\n",
    "        \"val_predictions\": val_predictions,\n",
    "        \"val_labels\": val_label,\n",
    "        \"val_edges\": val_edges,\n",
    "        \"test_edges\": test_edges}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1f56822-8b3b-429d-930b-08afd7eb31dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 70628\n",
      "Number of edges: 70707\n",
      "Is the graph directed? False\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "train_graph = results[\"train_graph\"]\n",
    "\n",
    "num_nodes = train_graph.number_of_nodes()\n",
    "num_edges = train_graph.number_of_edges()\n",
    "is_directed = nx.is_directed(train_graph)\n",
    "\n",
    "print(f\"Number of nodes: {num_nodes}\")\n",
    "print(f\"Number of edges: {num_edges}\")\n",
    "print(f\"Is the graph directed? {is_directed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19d0ef16-cd42-40b9-9ab2-68b0b8182e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[70628, 1433], edge_index=[2, 70707], y=[8976], edge_weight=[70707], z=[70628], node_id=[70628], num_nodes=70628, batch=[70628], ptr=[8977])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a52b8fe5-9e3c-4fcb-bbb5-239f152d3d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1054"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results[\"test_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe213ddb-7fd1-4e47-bba0-6be5740e578e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results[\"val_predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4577ccbe-010a-47b3-b8bb-960bea8c0a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"val_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "058c2e76-3adc-4557-ad75-c0b563172b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 526)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(results[\"val_edges\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9882939b-0d56-4196-be9a-53746bfd624d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1054)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(results[\"test_edges\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65044cae-8c64-49dc-8176-403f117de26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,    0,    0,  ..., 1053, 1053, 1053])\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "for daat in test_loader:\n",
    "    print(daat.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d68cc222-a9fe-41d6-90cf-0a273fa00f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7939\n"
     ]
    }
   ],
   "source": [
    "print(len(daat.batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19e7366b-105a-4300-8fd3-314403e2b3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[7939, 1433], edge_index=[2, 7786], y=[1054], edge_weight=[7786], z=[7939], node_id=[7939], num_nodes=7939, edge_label_index=[2, 527], edge_label=[527])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20d31176-f8c7-4235-ae19-3b38f54b5c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the graph results\n",
    "train_data = next(iter(DataLoader(train_dataset, batch_size=len(train_dataset))))\n",
    "val_data = next(iter(DataLoader(val_dataset, batch_size=len(val_dataset))))\n",
    "test_data = next(iter(DataLoader(test_dataset, batch_size=len(test_dataset))))\n",
    "\n",
    "test_logits = model(test_data.z, test_data.edge_index, test_data.batch, test_data.x, None, test_data.node_id)\n",
    "test_predictions = test_logits.view(-1).sigmoid().detach().cpu().numpy()\n",
    "test_label = test_data.y.view(-1).cpu().to(torch.float)\n",
    "\n",
    "val_logits = model(val_data.z, val_data.edge_index, val_data.batch, val_data.x, None, val_data.node_id)\n",
    "val_predictions = val_logits.view(-1).sigmoid().detach().cpu().numpy()\n",
    "val_label = val_data.y.view(-1).cpu().to(torch.float)\n",
    "\n",
    "G = to_networkx(train_data, to_undirected=True)\n",
    "val_edges = val_data.edge_index.cpu().numpy()\n",
    "\n",
    "all_test_edges = test_data.edge_index.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eae3f0b6-be75-4e00-bc9a-8789d612f511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
